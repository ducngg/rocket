{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/THU-MIG/yolov10.git\n",
        "!pip install -q roboflow\n",
        "!pip install pyngrok --quiet\n",
        "!pip install fastapi --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ykf-ywHiurUC",
        "outputId": "98838d1c-573a-436c-c441-c490d20eb6bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for ultralytics (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.7/178.7 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OBJECT DETECTOR"
      ],
      "metadata": {
        "id": "Ez5iQwKbScqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLOv10\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from fastapi import FastAPI, Response, Depends\n",
        "from fastapi.responses import JSONResponse\n",
        "from fastapi.openapi.docs import get_swagger_ui_html\n",
        "from pydantic import BaseModel\n",
        "from typing import List, Optional\n",
        "import base64\n",
        "from io import BytesIO\n",
        "from fastapi import FastAPI\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "!gdown 1QOtPaA4djNCXlpWRVQIu8WJ9M5RePFIn -O detector.pt\n",
        "DETECTOR = YOLOv10(\"detector.pt\").to(device)"
      ],
      "metadata": {
        "id": "Iu0dJK63wwPW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6668404-0feb-4037-8946-66ab459e9665"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1QOtPaA4djNCXlpWRVQIu8WJ9M5RePFIn\n",
            "From (redirected): https://drive.google.com/uc?id=1QOtPaA4djNCXlpWRVQIu8WJ9M5RePFIn&confirm=t&uuid=721c7380-08da-490c-b9e4-ee1b64b0d259\n",
            "To: /content/detector.pt\n",
            "100% 33.5M/33.5M [00:00<00:00, 33.8MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# COUNTER"
      ],
      "metadata": {
        "id": "-YETVY0bSfB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supervision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7x4LDqWCezIE",
        "outputId": "5404a5ec-7cdb-4d0f-a854-9596d5bc04e9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting supervision\n",
            "  Downloading supervision-0.21.0-py3-none-any.whl (123 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/124.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.0/124.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from supervision) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.25.2)\n",
            "Requirement already satisfied: opencv-python-headless>=4.5.5.64 in /usr/local/lib/python3.10/dist-packages (from supervision) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=9.4 in /usr/local/lib/python3.10/dist-packages (from supervision) (9.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /usr/local/lib/python3.10/dist-packages (from supervision) (6.0.1)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from supervision) (1.11.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6.0->supervision) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6.0->supervision) (1.16.0)\n",
            "Installing collected packages: supervision\n",
            "Successfully installed supervision-0.21.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from roboflow import Roboflow\n",
        "import supervision as sv\n",
        "\n",
        "rf = Roboflow(api_key=\"c4YSVfjlojQboqbkLeZ2\")\n",
        "project = rf.workspace().project(\"people-detection-phekv\")\n",
        "COUNTER = project.version(6).model"
      ],
      "metadata": {
        "id": "IUBrHikNSgQa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2bd11eca-23f1-4544-98f5-87ce5ba998c8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VDB"
      ],
      "metadata": {
        "id": "EmGpcgzLSadd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load pre-trained ResNet50 model\n",
        "resnet50 = models.resnet50(pretrained=True).to(device)\n",
        "resnet50.eval()\n",
        "\n",
        "# Remove the last layer (classification layer)\n",
        "resnet50 = nn.Sequential(*list(resnet50.children())[:-1]).to(device)\n",
        "resnet50.eval()\n",
        "\n",
        "# Define image preprocessing\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    image = preprocess(image).unsqueeze(0).to(device)\n",
        "\n",
        "    # torch.Tensor (1, C, W, H)\n",
        "    return image\n",
        "\n",
        "def get_embedding(image_path):\n",
        "    image = preprocess_image(image_path)\n",
        "    with torch.no_grad():\n",
        "        embedding = resnet50(image).cpu().numpy().flatten()\n",
        "\n",
        "    # numpy.ndarray (1 dim)\n",
        "    return embedding\n",
        "\n",
        "def preprocess_image_np(image_np):\n",
        "    # Example preprocessing: Normalize and resize\n",
        "    # Note: Adjust preprocessing as required for your model\n",
        "    image_np = image_np / 255.0  # Normalize to [0, 1]\n",
        "    image_np = np.transpose(image_np, (2, 0, 1))  # Change to (C, H, W) format\n",
        "    image_tensor = torch.tensor(image_np).float().unsqueeze(0)  # Add batch dimension\n",
        "    return image_tensor\n",
        "\n",
        "def get_embedding_np(image_np):\n",
        "    image_tensor = preprocess_image_np(image_np).to(device)\n",
        "    with torch.no_grad():\n",
        "        embedding = resnet50(image_tensor).cpu().numpy().flatten()\n",
        "\n",
        "    # numpy.ndarray (1 dim)\n",
        "    return embedding\n",
        "\n",
        "from collections import Counter\n",
        "import sys, time, os\n",
        "from dataclasses import dataclass, field\n",
        "from typing import List, Tuple\n",
        "from pydantic import BaseModel\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "@dataclass\n",
        "class Record:\n",
        "    source: str\n",
        "    brand: str\n",
        "    vector: np.ndarray\n",
        "\n",
        "class VectorDatabase:\n",
        "    def __init__(self):\n",
        "        self.db = {\n",
        "            \"bottle\": [],\n",
        "            \"can\": [],\n",
        "            \"carton\": [],\n",
        "            \"icebox\": [],\n",
        "            \"icebucket\": [],\n",
        "            \"standee\": [],\n",
        "            \"banner\": []\n",
        "        }\n",
        "\n",
        "    def add(self, filepath: str):\n",
        "        filename = os.path.basename(filepath)\n",
        "        if filename in [\".DS_Store\", \".ipynb_checkpoints\"]:\n",
        "            return\n",
        "\n",
        "        # tiger.can.1.png\n",
        "        brand, type, no, _ = filename.split('.')\n",
        "        vector = get_embedding(filepath)\n",
        "\n",
        "        if type == \"logo\":\n",
        "            for key in self.db.keys():\n",
        "                self.db[key].append(\n",
        "                    Record(filepath, brand, vector)\n",
        "                )\n",
        "        else:\n",
        "            if type not in self.db.keys():\n",
        "                raise ValueError(f\"Invalid type: {type}\")\n",
        "            self.db[type].append(\n",
        "                Record(filepath, brand, vector)\n",
        "            )\n",
        "\n",
        "    def brand(self, type, vector, threshold: float = 0.5, top_k: int = 5) -> str:\n",
        "        query_vector = vector.reshape(1, -1)  # Encode -> 2D array\n",
        "        record_vectors = np.array([record.vector for record in self.db[type]])\n",
        "\n",
        "        similarities = cosine_similarity(query_vector, record_vectors).flatten()\n",
        "        top_indices: List[int] = similarities.argsort()[::-1]\n",
        "\n",
        "        brand = self.db[type][top_indices[0]].brand\n",
        "\n",
        "        # Filter out indices with similarity below the threshold\n",
        "        top_indices = [idx for idx in top_indices if similarities[idx] > threshold]\n",
        "\n",
        "        if not top_indices:\n",
        "            return None  # No similar vectors above the threshold\n",
        "\n",
        "        # Get the top_k indices\n",
        "        top_k_indices = top_indices[:top_k]\n",
        "\n",
        "        for idx in top_k_indices:\n",
        "            print(similarities[idx], self.db[type][idx].brand, self.db[type][idx].source)\n",
        "\n",
        "        # Get the brands of the top_k most similar vectors\n",
        "        top_k_brands = [self.db[type][idx].brand for idx in top_k_indices]\n",
        "\n",
        "        # Determine the most common brand using majority vote\n",
        "        most_common_brand, _ = Counter(top_k_brands).most_common(1)[0]\n",
        "\n",
        "        return most_common_brand\n",
        "\n",
        "!gdown 1E0hTcVVj8deC6To3SSRVT-z7-J4d-G-n -O vdb.pkl\n",
        "VDB = pickle.load(open(\"vdb.pkl\", \"rb\"))\n"
      ],
      "metadata": {
        "id": "UpftnUbYJvFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e256b090-4b75-46bf-a9fe-4e4ffafd60ee"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 107MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1E0hTcVVj8deC6To3SSRVT-z7-J4d-G-n\n",
            "To: /content/vdb.pkl\n",
            "\r  0% 0.00/1.46M [00:00<?, ?B/s]\r100% 1.46M/1.46M [00:00<00:00, 50.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM"
      ],
      "metadata": {
        "id": "svIY6x6cx-B1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from agent import describer\n",
        "from PIL import Image\n",
        "import io\n",
        "import os\n",
        "import base64\n",
        "import cv2\n",
        "\n",
        "import google.generativeai as genai\n",
        "import os\n",
        "from IPython.display import Markdown\n",
        "\n",
        "GOOGLE_API_KEY = \"AIzaSyCLxGD1bfB7GaoTcI6vVYmzZavZT6VFhuI\"\n",
        "GEMINI = genai.GenerativeModel(model_name=\"gemini-1.5-pro-latest\")\n",
        "\n",
        "def crop_objects(img_path):\n",
        "    cropped_images = []\n",
        "\n",
        "    image = cv2.imread(img_path)\n",
        "    # TODO: add model yolov10\n",
        "    # expect output in format [{class, bbox: [x1, y1, x2, y2]}]\n",
        "    detections = model.predict(img_path)\n",
        "\n",
        "    for detection in detections:\n",
        "        class_id = detection['class']\n",
        "        x1, y1, x2, y2 = detection['bbox']\n",
        "\n",
        "        # Crop the image\n",
        "        cropped_img = image[y1:y2, x1:x2]\n",
        "        cropped_images.append({'class': class_id, 'image': cropped_img, 'bbox':detection['bbox']})\n",
        "\n",
        "    return cropped_images\n",
        "\n",
        "def detect_logo(img_path, obj):\n",
        "    # return {class, logo: bool}\n",
        "    pass\n",
        "\n",
        "\n",
        "def count_logo(img_path):\n",
        "    cropped_images = crop_object(img_path)\n",
        "\n",
        "    banner, bottle, can, carton, icebox, icebucket = 0\n",
        "    total_banner, total_bottle, total_can, total_carton, total_icebox, total_icebucket = 0\n",
        "\n",
        "    heineken_obj_bbox = []\n",
        "\n",
        "    for obj in cropped_images:\n",
        "        # TODO: add model detect logo\n",
        "        result = detect_logo(img_path, obj)\n",
        "\n",
        "        if result['class'] == \"banner\":\n",
        "            total_banner += 1\n",
        "            if result['logo'] == True:\n",
        "                banner += 1\n",
        "                heineken_obj_bbox.append(obj['bbox'])\n",
        "        elif result['class'] == \"bottle\":\n",
        "            total_bottle += 1\n",
        "            if result['logo'] == True:\n",
        "                bottle += 1\n",
        "                heineken_obj_bbox.append(obj['bbox'])\n",
        "        elif result['class'] == \"can\":\n",
        "            toal_can += 1\n",
        "            if result['logo'] == True:\n",
        "                can += 1\n",
        "                heineken_obj_bbox.append(obj['bbox'])\n",
        "        elif result['class'] == \"carton\":\n",
        "            total_carton += 1\n",
        "            if result['logo'] == True:\n",
        "                carton += 1\n",
        "                heineken_obj_bbox.append(obj['bbox'])\n",
        "        elif result['class'] == \"icebox\":\n",
        "            toal_icebox += 1\n",
        "            if result['logo'] == True:\n",
        "                icebox += 1\n",
        "                heineken_obj_bbox.append(obj['bbox'])\n",
        "        elif result['class'] == \"icebucket\":\n",
        "            total_icebucket += 1\n",
        "            if result['logo'] == True:\n",
        "                icebucket += 1\n",
        "                heineken_obj_bbox.append(obj['bbox'])\n",
        "\n",
        "    return [f'{banner}/{total_banner}', f'{bottle}/{total_bottle}', f'{can}/{total_can}',\n",
        "            f'{carton}/{total_carton}', f'{icebox}/{total_icebox}', f'{icebucket}/{total_icebucket}', heineken_obj_bbox]\n",
        "\n",
        "\n",
        "def upload_image(image_path):\n",
        "        # Upload the file and print a confirmation.\n",
        "    sample_file = genai.upload_file(path=image_path,\n",
        "                                display_name=\"test\")\n",
        "\n",
        "    print(f\"Uploaded file '{sample_file.display_name}' as: {sample_file.uri}\")\n",
        "    return sample_file\n",
        "\n",
        "def get_image_context(sample_file):\n",
        "\n",
        "    # Choose a Gemini API model.\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-1.5-pro-latest\")\n",
        "\n",
        "    # Prompt the model with text and the previously uploaded image.\n",
        "    response = model.generate_content([sample_file, \"Classify the image context as either 'grocery' or 'eating or drinking place', response with one context\"])\n",
        "    print(f'The context is {response.text}')\n",
        "    return response.text\n",
        "\n",
        "def check_grocery(count_logo, sample_file):\n",
        "    # Choose a Gemini API model.\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-1.5-pro-latest\")\n",
        "\n",
        "    # Prompt the model with text and the previously uploaded image.\n",
        "    response = model.generate_content([sample_file, f\"Based on {count_logo}, check if the appearance of heineken products is enough to make impact on people look at this grocery\"])\n",
        "    print(f'The context is {response.text}')\n",
        "    return response.text\n",
        "\n",
        "def check_restaurant(count_logo, count_people, sample_file):\n",
        "    # Choose a Gemini API model.\n",
        "    model = genai.GenerativeModel(model_name=\"gemini-1.5-pro-latest\")\n",
        "\n",
        "    # Prompt the model with text and the previously uploaded image.\n",
        "    response = model.generate_content([sample_file, f\"Based on {count_logo} and number of people is {count_people}, check if the appearance of heineken products is enough to make impact on people look at this restaurant\"])\n",
        "    print(f'The context is {response.text}')\n",
        "    return response.text"
      ],
      "metadata": {
        "id": "vHgE3I58x_PB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Deploy"
      ],
      "metadata": {
        "id": "Srxyzn5ux_p-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NGROK_TOKEN = \"2g3RAg7ZgYDcYCYfdhXV4qrxnju_S1W2e7QPURc8umyu28b4\""
      ],
      "metadata": {
        "id": "XLoiT1S03Cs-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import HTTPException\n",
        "import cv2"
      ],
      "metadata": {
        "id": "xKFEYKVwhTr9"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProcessInput(BaseModel):\n",
        "    image: str # Base 64\n",
        "\n",
        "class CheckRestaurantInput(BaseModel):\n",
        "    preds: dict # JSON\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/process\")\n",
        "async def process(item: ProcessInput):\n",
        "    try:\n",
        "        # Decode base64 image\n",
        "        image_data = base64.b64decode(item.image)\n",
        "        image = Image.open(BytesIO(image_data)).convert(\"RGB\")\n",
        "\n",
        "        # Convert PIL image to numpy array\n",
        "        image_np = np.array(image)\n",
        "\n",
        "        # Ensure the image has the correct shape (height, width, channels)\n",
        "        if len(image_np.shape) != 3 or image_np.shape[2] != 3:\n",
        "            raise ValueError(\"Invalid image shape. Expected (height, width, 3).\")\n",
        "\n",
        "        # Resize image to be divisible by 32\n",
        "        height, width = image_np.shape[:2]\n",
        "        new_height = (height // 32) * 32\n",
        "        new_width = (width // 32) * 32\n",
        "        image_resized = cv2.resize(image_np, (new_width, new_height))\n",
        "\n",
        "        # Perform inference with YOLO model\n",
        "        image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).float().to(device)\n",
        "        image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        # Perform inference with YOLO model\n",
        "        prediction = DETECTOR(image_tensor)[0]\n",
        "\n",
        "        names = prediction.names\n",
        "        prediction = prediction.boxes\n",
        "        h, w = prediction.orig_shape\n",
        "        class_predicted = prediction.cls.tolist()\n",
        "        confidence_predicted = prediction.conf.tolist()\n",
        "        xyxyn = np.round(np.array(prediction.xyxyn.tolist()), 3)\n",
        "\n",
        "        objects_predicted = []\n",
        "        for [x1, y1, x2, y2], confidence, cls in zip(xyxyn, confidence_predicted, class_predicted):\n",
        "            object_slice = image_np[int(y1*h):int(y2*h), int(x1*w):int(x2*w)]\n",
        "            vector = get_embedding_np(object_slice)\n",
        "            brand = VDB.brand(names[cls], vector)\n",
        "\n",
        "            if brand is None:\n",
        "                brand = \"other\"\n",
        "            objects_predicted.append({\n",
        "                'class_idx': int(cls),\n",
        "                'class': names[cls],\n",
        "                'brand': brand,\n",
        "                'confidence': confidence,\n",
        "                'box': {\n",
        "                    'x1': x1,\n",
        "                    'y1': y1,\n",
        "                    'x2': x2,\n",
        "                    'y2': y2\n",
        "                }\n",
        "            })\n",
        "\n",
        "            color = (255, 0, 0) if int(cls) % 2 == 0 else (0, 0, 255)\n",
        "\n",
        "            label = f\"{names[cls]}: {brand}\"\n",
        "            cv2.rectangle(image_resized, (int(x1*w), int(y1*h)), (int(x2*w), int(y2*h)), color, 1)\n",
        "            cv2.putText(image_resized, label, (int(x1*w), int(y1*h) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
        "\n",
        "        image_with_boxes = Image.fromarray(image_resized)\n",
        "        buffered = BytesIO()\n",
        "        image_with_boxes.save(buffered, format=\"JPEG\")\n",
        "        drawn_image_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "\n",
        "        preds = json.dumps({\n",
        "            'image': {\n",
        "                'width': w,\n",
        "                'height': h\n",
        "            },\n",
        "            'predictions': objects_predicted\n",
        "        }, indent=4)\n",
        "\n",
        "        return {\n",
        "            \"predictions\": preds,\n",
        "            \"drawn_image\": drawn_image_base64\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        raise HTTPException(status_code=400, detail=str(e))\n",
        "\n",
        "@app.post(\"/check-restaurant\")\n",
        "async def check_restaurant(item: CheckRestaurantInput):\n",
        "    try:\n",
        "        # Decode base64 image\n",
        "        image_data = base64.b64decode(item.image)\n",
        "        image = Image.open(BytesIO(image_data)).convert(\"RGB\")\n",
        "\n",
        "        # Convert PIL image to numpy array\n",
        "        image_np = np.array(image)\n",
        "\n",
        "        # Ensure the image has the correct shape (height, width, channels)\n",
        "        if len(image_np.shape) != 3 or image_np.shape[2] != 3:\n",
        "            raise ValueError(\"Invalid image shape. Expected (height, width, 3).\")\n",
        "\n",
        "        # Resize image to be divisible by 32\n",
        "        height, width = image_np.shape[:2]\n",
        "        new_height = (height // 32) * 32\n",
        "        new_width = (width // 32) * 32\n",
        "        image_resized = cv2.resize(image_np, (new_width, new_height))\n",
        "\n",
        "        # Perform inference with YOLO model\n",
        "        image_tensor = torch.from_numpy(image_resized).permute(2, 0, 1).float().to(device)\n",
        "        image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "        result = COUNTER.predict(image_tensor, confidence=40, overlap=30).json()\n",
        "        detections = sv.Detections.from_roboflow(result)\n",
        "        n_people = len(detections)\n",
        "\n",
        "        preds = json.loads(item.preds)\n",
        "\n",
        "        # further analysis by prompt\n",
        "        return n_people\n",
        "    except:\n",
        "        return 0\n"
      ],
      "metadata": {
        "id": "Y9bcy2c53rI3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2g3RAg7ZgYDcYCYfdhXV4qrxnju_S1W2e7QPURc8umyu28b4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57DhOOU5fkBu",
        "outputId": "fce64a77-a8e1-4240-d6c2-144159216afd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "from pyngrok import ngrok\n",
        "import re\n",
        "\n",
        "PORT = 5000\n",
        "\n",
        "ngrok_config = {\n",
        "    \"addr\": f\"{PORT}\",\n",
        "}\n",
        "public_url = ngrok.connect(**ngrok_config)\n",
        "print(' * Tunnel URL:', public_url)\n",
        "\n",
        "url_pattern = r'\"(https?://[^\"]+)\"'\n",
        "matches = re.findall(url_pattern, str(public_url))\n",
        "public_link = matches[0]\n",
        "\n",
        "print(f\"Endpoint: {public_link}/\")\n",
        "\n",
        "import uvicorn\n",
        "def run_server():\n",
        "    uvicorn.run(app, host='0.0.0.0', port=PORT)\n",
        "\n",
        "# Run the server in a separate thread\n",
        "server_thread = threading.Thread(target=run_server)\n",
        "server_thread.start()"
      ],
      "metadata": {
        "id": "_SNHruYA35mb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3084cc61-4675-4740-efc1-e997b8f869b6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Tunnel URL: NgrokTunnel: \"https://5eb9-34-124-209-108.ngrok-free.app\" -> \"http://localhost:5000\"\n",
            "Endpoint: https://5eb9-34-124-209-108.ngrok-free.app/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing purpose"
      ],
      "metadata": {
        "id": "0ykyLNAd36Sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "OBJ = {\n",
        "    \"image\": {\n",
        "        \"width\": 1242,\n",
        "        \"height\": 2208\n",
        "    },\n",
        "    \"predictions\": [\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.9188112616539001,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.638,\n",
        "                \"y1\": 0.174,\n",
        "                \"x2\": 0.999,\n",
        "                \"y2\": 0.496\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.9124937653541565,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.143,\n",
        "                \"y1\": 0.795,\n",
        "                \"x2\": 0.302,\n",
        "                \"y2\": 0.999\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 39,\n",
        "            \"class\": \"bottle\",\n",
        "            \"confidence\": 0.7329253554344177,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.361,\n",
        "                \"y1\": 0.364,\n",
        "                \"x2\": 0.398,\n",
        "                \"y2\": 0.412\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.7088688015937805,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.352,\n",
        "                \"y1\": 0.813,\n",
        "                \"x2\": 0.478,\n",
        "                \"y2\": 0.993\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.6062883734703064,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.135,\n",
        "                \"y1\": 0.228,\n",
        "                \"x2\": 0.252,\n",
        "                \"y2\": 0.366\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.5942355394363403,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.541,\n",
        "                \"y1\": 0.188,\n",
        "                \"x2\": 0.68,\n",
        "                \"y2\": 0.427\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 39,\n",
        "            \"class\": \"bottle\",\n",
        "            \"confidence\": 0.5817359089851379,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.321,\n",
        "                \"y1\": 0.36,\n",
        "                \"x2\": 0.36,\n",
        "                \"y2\": 0.424\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.5794417858123779,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.348,\n",
        "                \"y1\": 0.244,\n",
        "                \"x2\": 0.454,\n",
        "                \"y2\": 0.356\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.45053043961524963,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.864,\n",
        "                \"y1\": 0.731,\n",
        "                \"x2\": 0.998,\n",
        "                \"y2\": 0.974\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 25,\n",
        "            \"class\": \"umbrella\",\n",
        "            \"confidence\": 0.36702853441238403,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.501,\n",
        "                \"y1\": 0.0,\n",
        "                \"x2\": 0.777,\n",
        "                \"y2\": 0.165\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.3543442487716675,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.001,\n",
        "                \"y1\": 0.236,\n",
        "                \"x2\": 0.061,\n",
        "                \"y2\": 0.417\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.35057303309440613,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.067,\n",
        "                \"y1\": 0.242,\n",
        "                \"x2\": 0.15,\n",
        "                \"y2\": 0.356\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.3384465277194977,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.458,\n",
        "                \"y1\": 0.78,\n",
        "                \"x2\": 0.501,\n",
        "                \"y2\": 0.869\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 60,\n",
        "            \"class\": \"dining table\",\n",
        "            \"confidence\": 0.3155713677406311,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.003,\n",
        "                \"y1\": 0.343,\n",
        "                \"x2\": 0.503,\n",
        "                \"y2\": 0.495\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.3105430603027344,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.003,\n",
        "                \"y1\": 0.227,\n",
        "                \"x2\": 0.069,\n",
        "                \"y2\": 0.389\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 0,\n",
        "            \"class\": \"person\",\n",
        "            \"confidence\": 0.30441614985466003,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.0,\n",
        "                \"y1\": 0.8,\n",
        "                \"x2\": 0.034,\n",
        "                \"y2\": 0.904\n",
        "            }\n",
        "        },\n",
        "        {\n",
        "            \"class_idx\": 39,\n",
        "            \"class\": \"bottle\",\n",
        "            \"confidence\": 0.3034815788269043,\n",
        "            \"box\": {\n",
        "                \"x1\": 0.271,\n",
        "                \"y1\": 0.372,\n",
        "                \"x2\": 0.316,\n",
        "                \"y2\": 0.43\n",
        "            }\n",
        "        }\n",
        "    ]\n",
        "}"
      ],
      "metadata": {
        "id": "g6lqR4lYwGlu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}